---
layout: distill
title: "Lecture 7: Maximum likelihood learning of undirected GM" 
description: An example of a distill-style lecture notes that showcases the main elements.
date: 2019-01-09

lecturers:
  - name: Eric Xing
    url: "https://www.cs.cmu.edu/~epxing/"

authors:
  - name: Danish Danish  # author's full name
    url: "#"  # optional URL to the author's homepage
  - name: Tze Hui Koh
    url: "#"
  - name: Ramesh Balaji
    url: "#"
  - name: Shalom Yiblet
    url: "#"

editors:
  - name: Editor 1  # editor's full name
    url: "#"  # optional URL to the editor's homepage

abstract: >
  An example abstract block.
---

## Generalized Iterative Scaling (GIS): Introduction

Now this second algorithm is where we start thinking more about feature-based models. So far, when we mention potential functions, we feel that it is trivial. Either you learn it, or you simply specify numbers at will. But there are some problems. To illustrate this, let's take a look at a real task of spelling checking. One of the most useful and powerful devices in spelling checking is to build affinity models of character strings. 

### Feature-based Clique Potentials
For example, if we were to look at the consecutive appearance of 3 characters, $c_1, c_2, c_3$, which we represent with the potential function $\psi(x_c)$. The rationale behind this is that we can use this to score the likelihood of misspelling. E.g. If we see the sequence 'zyz', we would know that such a sequence does not exist in the English vocabulary and therefore has a very high likelihood of being a misspelling. On the other hand if we see the sequence 'ink', we know that there are some words which contain this sequence and consequently a lower likelihood of misspelling. This is an example of how the potential can help us do spelling checking. However if we were to iterate over all the different combinations of triplets in the alphabet, this would imply that we have $26^3$ features, and this gets worse if we increase the size of the sequence. Such a general "tabular" potential function would be exponentially costly for inference and have exponential numbers of parameters that we must learn from limited data, which would exhaust the computer's memory. 

One solution is to change the graphical model to make cliques smaller. But this changes the dependencies, and may force us to make more independence assumptions than we would like. Another solution is to keep the same graphical model, but use a less general parameterization of the clique potentials. And this is the idea behind feature-based models. 

### Features
In our earlier example of spelling checking, since we know how frequent certain triplets are based on existing knowledge (e.g. a dictionary or our own vocabulary), we can pick out those we have the gratest confidence of, and give them a feature, where a feature is defined as a function which is vacuous over all joint settings except a few particular ones which it is high or low. E.g. we can define a feature $f_{ing} = 10$ and $f_{ate} = 9$, and so on and so forth. And by the time we get to the 50-100th feature we assume that these combinations are less likely and assign them some arbitrary small likelihood of $\alpha$. We can also define features when the inputs are continuous. Then the idea of a cell on which it is active disappears, but we might still have a compact parameterization of the feature. 

This is a different approach from deep learning where we become overcomplete and hope that the big data will funnel in to get these numbers into place, with no guarantee, because we don't know how much data is necessary to get the numbers into place, i.e. we don't know the sample complexity. Instead, here we have a way in which we utilize human knowledge to build a model. And even today, the most successful speech recognition models have their foundations based on this concept.

### Features as Micropotentials
Let's move on to the details. By exponentiating them, each feature function can be made into a "micropotential". We can multiply these micropotentials together to get a clique potential. For example, a clique potential $\psi(c_1,c_2,c_2)$ could be expressed as:

<d-math block>
\begin{aligned}
\psi_c(c_1,c_2,c_3) = e^{\theta_{ing}f_{ing}} \times e^{\theta_{?ed}f_{?ed}} \times ... \\
  exp\{\sum^K_{k=1}\theta_k f_k(c_1,c_2,c_3)\}
\end{aligned}
</d-math>

There's still a potential over $26^3$ possible settings, but only uses $K$ parameters if there are $K$ features, and by having one indicator function per combination of $x_c$, we recover the standard tabular potential. 

### Combining Features
Note that these features can be handcrafted. We also append weights $\theta_k$ in the function because we do not have contextual knowledge of how important each feature is. The marginal over the clique is a generalized exponential family distribution, actually, a GLIM:

<d-math block>
\begin{aligned}
\psi_c(c_1,c_2,c_3) \propto exp\{\theta_{ing}f_{ing}(c_1,c_2,c_3) + \theta_{qu?}f_{qu?}(c_1,c_2,c_3) +\theta_{zzz}f_{zzz}(c_1,c_2,c_3) + ... \}
\end{aligned}
</d-math>

in general, the features may be overlapping, unconstrained indicators or any function of any subset of the clique variables. For example if you have multiple cliques such as overlapping windows over all possible combinations of triplets in our spelling checking example, this does not changed the anatomy of the potential, where we still have a weighted sum of the features.

$$
\psi_c(x_c) := exp\{\sum_{i\in I_c}\theta_k f_k(x_{c_i})\}
$$

### Feature Based Model
We can multiply these clique potentials as usual:

$$
p(x) = \frac{1}{Z(\theta)}\prod_c \psi_c(x_c) = \frac{1}{Z(\theta)}exp\{\sum_c \sum_{i \in I_c} \theta_k f_k(x_{c_i})\}
$$

However, in general, we can forget about associating features with cliques and just use a simplified form:

$$
p(x) = \frac{1}{Z(\theta)}exp\{\sum_{i} \theta_i f_i(x_{c_i})\} 
$$

With this redesign, we have the exponential family model, with the features as sufficient statistics. And so we get compactness and prior knowledge. However, the difficulty of learning is greater. Recall that in IPF, we have:

$$
\psi_c^{(t+1)}(x_c) = \psi_c^{(t)} \frac{\tilde{p}(x_c)}{p^{(t)}(x_c)}
$$

All we know is that the entire thing can be reweighted, iteratively, but here we have a combination of $\theta$ and $f$, and it is not obvious how we may use this rule to update the weights and features individually.

### MLE of Feature Based UGMs
Here let us look at our loss function (scaled likelihood):

<d-math block>
\begin{aligned}
\tilde{l}(\theta; D) = l(\theta;D)/N = \frac{1}{N}\sum_{n} \log p(x_n \mid \theta) \\
  = \sum_x \tilde{p}(x)\log p(x \mid \theta) \\
  = \sum_x \tilde{p}(x)\sum_i \theta_{i} f_i(x) - \log Z(\theta)
\end{aligned}
</d-math>


We introduce $N$ to allow the introduction of an empirical distribution. This is a techinicality we can put aside, because $N$ is a constant and does not change the relative numbers in our estimate. The loss function breaks down into two terms where one is a naive sum of $\theta f(x)$, and the other is the more complex $\log Z(\theta)$ with $\theta$ buried deeply into a compound. This is something that we cannot easily take the derivative with respect to. 

So instead of attacking this objective directly, we attack its lower bound. The rationale is that often in machine learning we linearize what is non-linear to reduce the complexity. Here we make use of the property that

$$
\log Z(\theta) \leq \mu Z(\theta) - \log\mu -1
$$

This bound holds for all $\mu$, in particular, it also holds for $\mu = Z^{-1}(\theta^{(t)})$. As a result, we have

$$
\tilde{l}(\theta; D) \geq \sum_x \tilde{p}(x) \sum_i \theta_i f_i(x) - \frac{Z(\theta)}{Z(\theta^{(t)}} - \log Z(\theta^{(t)}) + 1
$$

Where we assume that there exists a previous version $\theta^{(t)}$.

## Generalized Iterative Scaling: Algorithm
To make our task more explicit, let us establish an explicit relationship; the desired version of $\theta$ versus the current version of $\theta$, called $\Delta\theta$, such that $\Delta \theta_i^{(t)} := \theta_i - \theta_i^{(t)}$. We substitute this into the lower bound of the scaled loglikelihood.

Unfortunately, this is still a difficult expression to deal with. Here $\theta$ is compounded by the summation over all $\theta$, and then exponentiated. It becomes non-linear, and every $\theta$ is coupled with every other $\theta$ because of this operation. But this function has a specific form. It is the exponential of a sum or weighted sum. Let's switch perspective and treat $f$ as the weights and $\Delta \theta$ as the arguments.

With the assumptions

$$
f_i(x) \geq 0, \sum_i f_i(x) = 1
$$

The convexity of the exponential is such that

$$
exp(\sum_i \pi_i x_i) \leq \sum_i \pi_i exp(x_i)
$$

And now we have the sum of the exponentials of each individual $\theta$. This is a commonly used algebriac trick in machine learning.

$$
\tilde{l}(\theta;D) \geq \sum_i \theta_i \sum_x \tilde{p}(x)f_i(x) - \sum_x p(x \mid \theta^{(t)}) \sum_i f_i(x)exp(\Delta \theta_i^{(t)}) - \log Z(\theta^{(t)})+1 := \Lambda (\theta)
$$

This is known as GIS. So instead of using the original scaled loglikelihood, we use the lower bound of that which we call $\Lambda (\theta)$. Subsequently we can then take the following standard steps-

Take derivative:

$$
\frac{\delta \Lambda}{\delta \theta_i} = \sum_x \tilde{p}(x)f_i(x) - exp(\Delta \theta_i^{(t)})\sum_x p(x \mid \theta^{(t)})f_i(x)
$$

Set the derivative to zero

$$
e^{\Delta \theta_i^{(t)}} = \frac{\sum_x \tilde{p}(x)f_i(x)}{\sum_x p(x\mid \theta^{(t)})f_i(x)} = \frac{\sum_x \tilde{p}(x)f_i(x)}{\sum_x p^{(t)}(x)f_i(x)}Z(\theta^{(t)})
$$

Where $p^{(t)}(x)$ is the unnormalized version of $p(x \mid \theta^{(t)})$

$$
\theta_i^{(t+1)} = \theta_i^{(t)} + \Delta \theta_i^{(t)} \Rightarrow p^{(t+1)}(x) = p^{(t)}(x)\prod_i e^{(\Delta \theta_i^{(t)}f_i(x)}
$$

We now have the update equations

<d-math block>
\begin{aligned}
p^{(t+1)}(x) = \frac{p^{(t)}(x)}{Z(\theta^{(t)})}\prod_i( \frac{\sum_x \tilde{p}(x)f_i(x)}{\sum_x p^{(t)}(x)f_i(x)}Z(\theta^{(t)}))^{f_i(x)} \\
  = \frac{p^{(t)}(x)}{Z(\theta^{(t)})}\prod_i( \frac{\sum_x \tilde{p}(x)f_i(x)}{\sum_x p^{(t)}(x)f_i(x)})^{f_i(x)}(Z(\theta^{(t)}))^{\sum_i f_i(x)} \\
  = p^{(t)}(x)\prod_i( \frac{\sum_x \tilde{p}(x)f_i(x)}{\sum_x p^{(t)}(x)f_i(x)})^{f_i(x)}
\end{aligned}
</d-math>

Why do we call this scaling? To see this, take a look at the form of the last equation. It is in the form of a ratio calculated over two methods. One is the feature weighted by its empirical probability, and the other is a feature weighted by the inferred probability. In other words it's the ratio between what you observe and what you estimate based on what you have now, and we are essentially rescaling the expression iteratively like this. 

## Summary of GIS and IPF
In sum, IPF is a general algorithm for finding the MLE of UGMs. It has a fixed point equation for $\psi_c$ single cliques which do not have to be max-cliques, I-projection in the clique marginal space, requires the potential to be fully parameterized, and in the case of a fully decomposable model reduces to a single step iteration.

On the other hand, GIS is an iterative scaling on general UGM with feature-based potentials. IPF is in fact a special case of GIS in which the clique potential is built on features defined as an indicator function of clique configurations.
