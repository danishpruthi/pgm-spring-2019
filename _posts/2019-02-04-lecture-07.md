---
layout: distill
title: "Lecture 7: Maximum likelihood learning of undirected GM" 
description: An example of a distill-style lecture notes that showcases the main elements.
date: 2019-01-09

lecturers:
  - name: Eric Xing
    url: "https://www.cs.cmu.edu/~epxing/"

authors:
  - name: Danish Danish  # author's full name
    url: "#"  # optional URL to the author's homepage
  - name: Tze Hui Koh
    url: "#"
  - name: Ramesh Balaji
    url: "#"
  - name: Shalom Yiblet
    url: "#"

editors:
  - name: Editor 1  # editor's full name
    url: "#"  # optional URL to the editor's homepage

abstract: >
  An example abstract block.
---

## Generalized Iterative Scaling (GIS): Introduction

Now this second algorithm is where we start thinking more about feature-based models. So far, when we mention potential functions, we feel that it is trivial. Either you learn it, or you simply specify numbers at will. But there are some problems. To illustrate this, let's take a look at a real task of spelling checking. One of the most useful and powerful devices in spelling checking is to build affinity models of character strings. 

### Feature-based Clique Potentials
For example, if we were to look at the consecutive appearance of 3 characters, $c_1, c_2, c_3$, which we represent with the potential function $\psi(x_c)$. The rationale behind this is that we can use this to score the likelihood of misspelling. E.g. If we see the sequence 'zyz', we would know that such a sequence does not exist in the English vocabulary and therefore has a very high likelihood of being a misspelling. On the other hand if we see the sequence 'ink', we know that there are some words which contain this sequence and consequently a lower likelihood of misspelling. This is an example of how the potential can help us do spelling checking. However if we were to iterate over all the different combinations of triplets in the alphabet, this would imply that we have $26^3$ features, and this gets worse if we increase the size of the sequence. Such a general "tabular" potential function would be exponentially costly for inference and have exponential numbers of parameters that we must learn from limited data, which would exhaust the computer's memory. 

One solution is to change the graphical model to make cliques smaller. But this changes the dependencies, and may force us to make more independence assumptions than we would like. Another solution is to keep the same graphical model, but use a less general parameterization of the clique potentials. And this is the idea behind feature-based models. 

### Features
In our earlier example of spelling checking, since we know how frequent certain triplets are based on existing knowledge (e.g. a dictionary or our own vocabulary), we can pick out those we have the gratest confidence of, and give them a feature, where a feature is defined as a function which is vacuous over all joint settings except a few particular ones which it is high or low. E.g. we can define a feature $f_{ing} = 10$ and $f_{ate} = 9$, and so on and so forth. And by the time we get to the 50-100th feature we assume that these combinations are less likely and assign them some arbitrary small likelihood of $\alpha$. We can also define features when the inputs are continuous. Then the idea of a cell on which it is active disappears, but we might still have a compact parameterization of the feature. 

This is a different approach from deep learning where we become overcomplete and hope that the big data will funnel in to get these numbers into place, with no guarantee, because we don't know how much data is necessary to get the numbers into place, i.e. we don't know the sample complexity. Instead, here we have a way in which we utilize human knowledge to build a model. And even today, the most successful speech recognition models have their foundations based on this concept.

### Features as Micropotentials
Let's move on to the details. By exponentiating them, each feature function can be made into a "micropotential". We can multiply these micropotentials together to get a clique potential. For example, a clique potential $\psi(c_1,c_2,c_2)$ could be expressed as:

<d-math block>
\begin{aligned}
\psi_c(c_1,c_2,c_3) = e^{\theta_{ing}f_{ing}} \times e^{\theta_{?ed}f_{?ed}} \times ... \\
  exp\{\sum^K_{k=1}\theta_k f_k(c_1,c_2,c_3)\}
\end{aligned}
</d-math>

There's still a potential over $26^3$ possible settings, but only uses $K$ parameters if there are $K$ features, and by having one indicator function per combination of $x_c$, we recover the standard tabular potential. 

### Combining Features
Note that these features can be handcrafted. We also append weights $\theta_k$ in the function because we do not have contextual knowledge of how important each feature is. The marginal over the clique is a generalized exponential family distribution, actually, a GLIM:

<d-math block>
\begin{aligned}
\psi_c(c_1,c_2,c_3) \propto exp\{\theta_{ing}f_{ing}(c_1,c_2,c_3) + \theta_{qu?}f_{qu?}(c_1,c_2,c_3) +\theta_{zzz}f_{zzz}(c_1,c_2,c_3) + ... \}
\end{aligned}
</d-math>

in general, the features may be overlapping, unconstrained indicators or any function of any subset of the clique variables. For example if you have multiple cliques such as overlapping windows over all possible combinations of triplets in our spelling checking example, this does not changed the anatomy of the potential, where we still have a weighted sum of the features.

$$
\psi_c(x_c) := exp\{\sum_{i\in I_c}\theta_k f_k(x_{c_i})\}
$$

### Feature Based Model
We can multiply these clique potentials as usual:

$$
p(x) = \frac{1}{Z(\theta)}\prod_c \psi_c(x_c) = \frac{1}{Z(\theta)}exp\{\sum_c \sum_{i \in I_c} \theta_k f_k(x_{c_i})\}
$$

However, in general, we can forget about associating features with cliques and just use a simplified form:

$$
p(x) = \frac{1}{Z(\theta)}exp\{\sum_{i} \theta_i f_i(x_{c_i})\} 
$$

With this redesign, we have the exponential family model, with the features as sufficient statistics. And so we get compactness and prior knowledge. However, the difficulty of learning is greater. Recall that in IPF, we have:

$$
\psi_c^{(t+1)}(x_c) = \psi_c^{(t)} \frac{\tilde{p}(x_c)}{p^{(t)}(x_c)}
$$

All we know is that the entire thing can be reweighted, iteratively, but here we have a combination of $\theta$ and $f$, and it is not obvious how we may use this rule to update the weights and features individually.

### MLE of Feature Based UGMs
Here let us look at our loss function (scaled likelihood):

<d-math block>
\begin{aligned}
\tilde{l}(\theta; D) = l(\theta;D)/N = \frac{1}{N}\sum_{n} \log p(x_n \mid \theta) \\
  = \sum_x \tilde{p}(x)\log p(x \mid \theta) \\
  = \sum_x \tilde{p}(x)\sum_i \theta_{i} f_i(x) - \log Z(\theta)
\end{aligned}
</d-math>


We introduce $N$ to allow the introduction of an empirical distribution. This is a techinicality we can put aside, because $N$ is a constant and does not change the relative numbers in our estimate. The loss function breaks down into two terms where one is a naive sum of $\theta f(x)$, and the other is the more complex $\log Z(\theta)$ with $\theta$ buried deeply into a compound. This is something that we cannot easily take the derivative with respect to. 

So instead of attacking this objective directly, we attack its lower bound. The rationale is that often in machine learning we linearize what is non-linear to reduce the complexity. Here we make use of the property that

$$
\log Z(\theta) \leq \mu Z(\theta) - \log\mu -1
$$

This bound holds for all $\mu$, in particular, it also holds for $\mu = Z^{-1}(\theta^{(t)})$. As a result, we have

$$
\tilde{l}(\theta; D) \geq \sum_x \tilde{p}(x) \sum_i \theta_i f_i(x) - \frac{Z(\theta)}{Z(\theta^{(t)}} - \log Z(\theta^{(t)}) + 1
$$

Where we assume that there exists a previous version $\theta^{(t)}$.

## Generalized Iterative Scaling: Algorithm
To make our task more explicit, let us establish an explicit relationship; the desired version of $\theta$ versus the current version of $\theta$, called $\Delta\theta$, such that $\Delta \theta_i^{(t)} := \theta_i - \theta_i^{(t)}$. We substitute this into the lower bound of the scaled loglikelihood.

Unfortunately, this is still a difficult expression to deal with. Here $\theta$ is compounded by the summation over all $\theta$, and then exponentiated. It becomes non-linear, and every $\theta$ is coupled with every other $\theta$ because of this operation. But this function has a specific form. It is the exponential of a sum or weighted sum. Let's switch perspective and treat $f$ as the weights and $\Delta \theta$ as the arguments.

With the assumptions

$$
f_i(x) \geq 0, \sum_i f_i(x) = 1
$$

The convexity of the exponential is such that

$$
exp(\sum_i \pi_i x_i) \leq \sum_i \pi_i exp(x_i)
$$

And now we have the sum of the exponentials of each individual $\theta$. This is a commonly used algebriac trick in machine learning.

$$
\tilde{l}(\theta;D) \geq \sum_i \theta_i \sum_x \tilde{p}(x)f_i(x) - \sum_x p(x \mid \theta^{(t)}) \sum_i f_i(x)exp(\Delta \theta_i^{(t)}) - \log Z(\theta^{(t)})+1 := \Lambda (\theta)
$$

This is known as GIS. So instead of using the original scaled loglikelihood, we use the lower bound of that which we call $\Lambda (\theta)$. Subsequently we can then take the following standard steps-

Take derivative:

$$
\frac{\delta \Lambda}{\delta \theta_i} = \sum_x \tilde{p}(x)f_i(x) - exp(\Delta \theta_i^{(t)})\sum_x p(x \mid \theta^{(t)})f_i(x)
$$

Set the derivative to zero to obtain update equations:
$$
e^{\Delta \theta_i^{(t)}} = 
$$
## Layouts

The main text column is referred to as the body.
It is the assumed layout of any direct descendants of the `d-article` element.

<div class="fake-img l-body">
  <p>.l-body</p>
</div>

For images you want to display a little larger, try `.l-page`:

<div class="fake-img l-page">
  <p>.l-page</p>
</div>

All of these have an outset variant if you want to poke out from the body text a little bit.
For instance:

<div class="fake-img l-body-outset">
  <p>.l-body-outset</p>
</div>

<div class="fake-img l-page-outset">
  <p>.l-page-outset</p>
</div>

Occasionally you’ll want to use the full browser width.
For this, use `.l-screen`.
You can also inset the element a little from the edge of the browser by using the inset variant.

<div class="fake-img l-screen">
  <p>.l-screen</p>
</div>
<div class="fake-img l-screen-inset">
  <p>.l-screen-inset</p>
</div>

The final layout is for marginalia, asides, and footnotes.
It does not interrupt the normal flow of `.l-body` sized text except on mobile screen sizes.

<div class="fake-img l-gutter">
  <p>.l-gutter</p>
</div>

***

## Arbitrary $$\LaTeX$$ (experimental)

In fact, you can write entire blocks of LaTeX using `<latex-js>...</latex-js>` tags.
Below is an example:<d-footnote>If you don't see anything, it means that your browser does not support Shadow DOM.</d-footnote>

<latex-js style="border: 1px dashed #aaa;">
This document will show most of the supported features of \LaTeX.js.

\section{Characters}

It is possible to input any UTF-8 character either directly or by character code
using one of the following:

\begin{itemize}
    \item \texttt{\textbackslash symbol\{"00A9\}}: \symbol{"00A9}
    \item \verb|\char"A9|: \char"A9
    \item \verb|^^A9 or ^^^^00A9|: ^^A9 or ^^^^00A9
\end{itemize}

\bigskip

\noindent
Special characters, like those:
\begin{center}
\$ \& \% \# \_ \{ \} \~{} \^{} \textbackslash % \< \>  \"   % TODO cannot be typeset
\end{center}
%
have to be escaped.

More than 200 symbols are accessible through macros. For instance: 30\,\textcelsius{} is
86\,\textdegree{}F.
</latex-js>

Note that you can easily interleave latex blocks with the standard markdown.

<latex-js style="border: 1px dashed #aaa;">
\section{Environments}

\subsection{Lists: Itemize, Enumerate, and Description}

The \texttt{itemize} environment is suitable for simple lists, the \texttt{enumerate} environment for
enumerated lists, and the \texttt{description} environment for descriptions.

\begin{enumerate}
    \item You can nest the list environments to your taste:
        \begin{itemize}
            \item But it might start to look silly.
            \item[-] With a dash.
        \end{itemize}
    \item Therefore remember: \label{remember}
        \begin{description}
            \item[Stupid] things will not become smart because they are in a list.
            \item[Smart] things, though, can be presented beautifully in a list.
        \end{description}
    \item Technical note: Viewing this in Chrome, however, will show too much vertical space
        at the end of a nested environment (see above). On top of that, margin collapsing for inline-block
        boxes is not allowed. Maybe using \texttt{dl} elements is too complicated for this and a simple nested
        \texttt{div} should be used instead.
\end{enumerate}
%
Lists can be deeply nested:
%
\begin{itemize}
  \item list text, level one
    \begin{itemize}
      \item list text, level two
        \begin{itemize}
          \item list text, level three

            And a new paragraph can be started, too.
            \begin{itemize}
              \item list text, level four

                And a new paragraph can be started, too.
                This is the maximum level.

              \item list text, level four
            \end{itemize}

          \item list text, level three
        \end{itemize}
      \item list text, level two
    \end{itemize}
  \item list text, level one
  \item list text, level one
\end{itemize}

\section{Mathematical Formulae}

Math is typeset using KaTeX. Inline math:
$
f(x) = \int_{-\infty}^\infty \hat f(\xi)\,e^{2 \pi i \xi x} \, d\xi
$
as well as display math is supported:
$$
f(n) = \begin{cases} \frac{n}{2}, & \text{if } n\text{ is even} \\ 3n+1, & \text{if } n\text{ is odd} \end{cases}
$$

</latex-js>

Full $$\LaTeX$$ blocks are supported through [LaTeX JS](https://latex.js.org/){:target="\_blank"} library, which is still under development and supports only limited functionality (which is still pretty cool!) and does not allow fine-grained control of the layout, fonts, etc.

*Note: We do not recommend using using LaTeX JS for writing lecture notes at this stage.*

***

## Print

Finally, you can easily get a PDF or printed version of the notes by simply hitting `ctrl+P` (or `⌘+P` on macOS).
