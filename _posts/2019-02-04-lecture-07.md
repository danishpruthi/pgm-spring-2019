---
layout: distill
title: "Lecture 7: Maximum likelihood learning of undirected GM"
description: "Algorithms for learning UGMs along with a brief overview of CRFs"
date: 2019-02-06

lecturers:
  - name: Eric Xing
    url: "https://www.cs.cmu.edu/~epxing/"

authors:
  - name: Ramesh Balaji
    url: "#"  # optional URL to the author's homepage
  - name: Tze Hui Koh
    url: "#"
  - name: Shalom Yiblet
    url: "http://yiblet.me"
  - name: Danish Danish
    url: "http://cs.cmu.edu/~ddanish"

editors:
  - name: Maruan Al-Shedivat
    url: "https://www.cs.cmu.edu/~mshediva/"  # optional URL to the editor's homepage

abstract: >
  An example abstract block.
---

## Generalized Iterative Scaling (GIS): Introduction

Now this second algorithm is where we start thinking more about feature-based models. So far, when we mention potential functions, we feel that it is trivial. Either you learn it, or you simply specify numbers at will. But there are some problems. To illustrate this, let's take a look at a real task of spelling checking. One of the most useful and powerful devices in spelling checking is to build affinity models of character strings.

_**Feature-based Clique Potentials**_:
For example, if we were to look at the consecutive appearance of 3 characters, $c_1, c_2, c_3$, which we represent with the potential function $\psi(x_c)$. The rationale behind this is that we can use this to score the likelihood of misspelling. E.g. If we see the sequence 'zyz', we would know that such a sequence does not exist in the English vocabulary and therefore has a very high likelihood of being a misspelling. On the other hand, if we see the sequence 'ink', we know that there are some words which contain this sequence and consequently a lower likelihood of misspelling. This is an example of how the potential can help us do spelling checking. However, if we were to iterate over all the different combinations of triplets in the alphabet, this would imply that we have $26^3$ features, and this gets worse if we increase the size of the sequence. Such a general "tabular" potential function would be exponentially costly for inference and have exponential numbers of parameters that we must learn from limited data, which would exhaust the computer's memory.

One solution is to change the graphical model to make cliques smaller. But this changes the dependencies, and may force us to make more independence assumptions than we would like. Another solution is to keep the same graphical model, but use a less general parameterization of the clique potentials. And this is the idea behind feature-based models.

_**Features**_:
In our earlier example of spelling checking, since we know how frequent certain triplets are based on the existing knowledge (e.g. a dictionary or our own vocabulary), we can pick out those we have the greatest confidence in and assign them features. Here a feature is defined as a function which is vacuous for most joint settings except a few particular ones, in which it returns either high or low values. E.g. we can define a feature $f_{ing} = 10$ and $f_{ate} = 9$, and so on and so forth. By the time we get to the 50th or 100th feature, we assume that these combinations are less likely and assign them some arbitrary small likelihood of $\alpha$. We can also define features when the inputs are continuous. Then the idea of having a cell on which the feature is active is no longer relevant, but we might still have a compact parameterization of the feature.

This is a different approach from deep learning where we become overcomplete and hope that the big data will funnel in to get these numbers into place, with no guarantee, because we don't know how much data is necessary to get the numbers into place, i.e. we don't know the sample complexity. Instead, here we have a way in which we utilize human knowledge to build a model. And even today, the most successful speech recognition models have their foundations based on this concept.

_**Features as Micropotentials**_:
Let's move on to the details. By exponentiating them, each feature function can be made into a "micropotential". We can multiply these micropotentials together to get a clique potential. For example, a clique potential $\psi(c_1,c_2,c_2)$ could be expressed as:

<d-math block>
\begin{aligned}
\psi_c (c_1,c_2,c_3) = e^{\theta_{ing} f_{ing}} \times e^{\theta_{?ed} f_{?ed}} \times ... \\
  &= \exp \left \{ \sum^K_{k=1} \theta_k f_k(c_1,c_2,c_3) \right \}
\end{aligned}
</d-math>

There's still a potential over $26^3$ possible settings, but only uses $K$ parameters if there are $K$ features, and by having one indicator function per combination of $x_c$, we recover the standard tabular potential.

_**Combining Features**_:
Note that these features can be handcrafted. We also append weights $\theta_k$ in the function because we do not have contextual knowledge of how important each feature is. The marginal over the clique is a generalized exponential family distribution, actually, a GLIM:

<d-math block>
\begin{aligned}
\psi_c (c_1,c_2,c_3) \propto \exp \left \{ \theta_{ing}f_{ing}(c_1,c_2,c_3) + \theta_{qu?}f_{qu?}(c_1,c_2,c_3) +\theta_{zzz}f_{zzz}(c_1,c_2,c_3) + \dots \right \}
\end{aligned}
</d-math>

In general, the features may be overlapping, unconstrained indicators or any function of any subset of the clique variables. For example if you have multiple cliques such as overlapping windows over all possible combinations of triplets in our spelling checking example, this does not changed the anatomy of the potential, where we still have a weighted sum of the features.

<d-math block>
\psi_c(x_c) := \exp \left \{ \sum_{i\in I_c} \theta_k f_k(x_{c_i}) \right \}
</d-math>

_**Feature Based Model**_:
We can multiply these clique potentials as usual:

<d-math block>
p(x) = \frac{1}{Z(\theta)} \prod_c \psi_c (x_c) = \frac{1}{Z(\theta)} \exp \left \{\sum_c \sum_{i \in I_c} \theta_k f_k(x_{c_i}) \right \}
</d-math>

However, in general, we can forget about associating features with cliques and just use a simplified form:

<d-math block>
p(x) = \frac{1}{Z(\theta)}\exp \left \{\sum_{i} \theta_i f_i(x_{c_i}) \right \}
</d-math>

With this redesign, we have the exponential family model, with the features as sufficient statistics. And so we get compactness and prior knowledge. However, the difficulty of learning is greater. Recall that in IPF, we have:

<d-math block>
\psi_c^{(t+1)} (x_c) = \psi_c^{(t)} \frac{\tilde{p}(x_c)}{p^{(t)}(x_c)}
</d-math>

All we know is that the entire thing can be reweighted, iteratively, but here we have a combination of $\theta$ and $f$, and it is not obvious how we may use this rule to update the weights and features individually.

_**MLE of Feature Based UGMs**_:
Here let us look at our loss function (scaled likelihood):

<d-math block>
\begin{aligned}
\tilde{l}(\theta ; D)
&= \frac{l(\theta ;D)}{N} \\
&= \frac{1}{N} \sum_{n} \log p(x_n \mid \theta) \\
&= \sum_x \tilde{p} (x)\log p(x \mid \theta) \\
&= \sum_x \tilde{p} (x)\sum_i \theta_{i} f_i(x) - \log Z(\theta)
\end{aligned}
</d-math>


We introduce $N$ to allow the introduction of an empirical distribution. This is a techinicality we can put aside, because $N$ is a constant and does not change the relative numbers in our estimate. The loss function breaks down into two terms where one is a naive sum of $\theta f(x)$, and the other is the more complex $\log Z(\theta)$ with $\theta$ buried deeply into a compound. This is something that we cannot easily take the derivative with respect to.

So instead of attacking this objective directly, we attack its lower bound. The rationale is that often in machine learning we linearize what is non-linear to reduce the complexity. Here we make use of the property that

<d-math block>
\log Z(\theta) \leq \mu Z(\theta) - \log\mu -1
</d-math>

This bound holds for all $\mu$, in particular, it also holds for $\mu = Z^{-1}(\theta^{(t)})$. As a result, we have

<d-math block>
\tilde{l}(\theta ; D) \geq \sum_x \tilde{p} (x) \sum_i \theta_i f_i(x) - \frac{Z(\theta)}{Z(\theta^{(t)}} - \log Z(\theta^{(t)}) + 1
</d-math>

Where we assume that there exists a previous version $\theta^{(t)}$.

## Generalized Iterative Scaling: Algorithm
To make our task more explicit, let us establish an explicit relationship; the desired version of $\theta$ versus the current version of $\theta$, called $\Delta\theta$, such that $\Delta \theta_i^{(t)} := \theta_i - \theta_i^{(t)}$. We substitute this into the lower bound of the scaled loglikelihood.

Unfortunately, this is still a difficult expression to deal with. Here $\theta$ is compounded by the summation over all $\theta$, and then exponentiated. It becomes non-linear, and every $\theta$ is coupled with every other $\theta$ because of this operation. But this function has a specific form. It is the exponential of a sum or weighted sum. Let's switch perspective and treat $f$ as the weights and $\Delta \theta$ as the arguments.

With the assumptions

<d-math block>
f_i(x) \geq 0, \sum_i f_i(x) = 1
</d-math>

The convexity of the exponential is such that

<d-math block>
exp(\sum_i \pi_i x_i) \leq \sum_i \pi_i exp(x_i)
</d-math>

And now we have the sum of the exponentials of each individual $\theta$. This is a commonly used algebriac trick in machine learning.

<d-math block>
\tilde{l}(\theta;D) \geq \sum_i \theta_i \sum_x \tilde{p}(x)f_i(x) - \sum_x p(x \mid \theta^{(t)}) \sum_i f_i(x)exp(\Delta \theta_i^{(t)}) - \log Z(\theta^{(t)})+1 := \Lambda (\theta)
</d-math>

This is known as GIS. So instead of using the original scaled loglikelihood, we use the lower bound of that which we call $\Lambda (\theta)$. Subsequently we can then take the following standard steps-

Take derivative:

<d-math block>
\frac{\delta \Lambda}{\delta \theta_i} = \sum_x \tilde{p}(x)f_i(x) - exp(\Delta \theta_i^{(t)})\sum_x p(x \mid \theta^{(t)})f_i(x)
</d-math>

Set the derivative to zero

<d-math block>
e^{\Delta \theta_i^{(t)}} = \frac{\sum_x \tilde{p}(x)f_i(x)}{\sum_x p(x\mid \theta^{(t)})f_i(x)} = \frac{\sum_x \tilde{p}(x)f_i(x)}{\sum_x p^{(t)}(x)f_i(x)}Z(\theta^{(t)})
</d-math>

Where $p^{(t)}(x)$ is the unnormalized version of $p(x \mid \theta^{(t)})$

<d-math block>
\theta_i^{(t+1)} = \theta_i^{(t)} + \Delta \theta_i^{(t)} \Rightarrow p^{(t+1)}(x) = p^{(t)}(x)\prod_i e^{(\Delta \theta_i^{(t)}f_i(x)}
</d-math>

We now have the update equations

<d-math block>
\begin{aligned}
p^{(t+1)}(x) &= \frac{p^{(t)}(x)}{Z(\theta^{(t)})}\prod_i( \frac{\sum_x \tilde{p}(x)f_i(x)}{\sum_x p^{(t)}(x)f_i(x)}Z(\theta^{(t)}))^{f_i(x)} \\
  &= \frac{p^{(t)}(x)}{Z(\theta^{(t)})}\prod_i( \frac{\sum_x \tilde{p}(x)f_i(x)}{\sum_x p^{(t)}(x)f_i(x)})^{f_i(x)}(Z(\theta^{(t)}))^{\sum_i f_i(x)} \\
  &= p^{(t)}(x)\prod_i( \frac{\sum_x \tilde{p}(x)f_i(x)}{\sum_x p^{(t)}(x)f_i(x)})^{f_i(x)}
\end{aligned}
</d-math>

Why do we call this scaling? To see this, take a look at the form of the last equation. It is in the form of a ratio calculated over two methods. One is the feature weighted by its empirical probability, and the other is a feature weighted by the inferred probability. In other words it's the ratio between what you observe and what you estimate based on what you have now, and we are essentially rescaling the expression iteratively like this.

## Summary of IPF and GIS
In sum, IPF is a general algorithm for finding the MLE of UGMs. It has a fixed point equation for $\psi_c$ single cliques which do not have to be max-cliques, I-projection in the clique marginal space, requires the potential to be fully parameterized, and in the case of a fully decomposable model reduces to a single step iteration.

On the other hand, GIS is an iterative scaling on general UGM with feature-based potentials. IPF is in fact a special case of GIS in which the clique potential is built on features defined as an indicator function of clique configurations.
