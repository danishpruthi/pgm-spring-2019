---
layout: distill
title: "Lecture 7: Maximum likelihood learning of undirected GM"
description: "A sample tagline for the lecture"
date: 2019-02-06

lecturers:
  - name: Eric Xing
    url: "https://www.cs.cmu.edu/~epxing/"

authors:
  - name: Ramesh Balaji
    url: "#"  # optional URL to the author's homepage
  - name: Tze Hui Koh
    url: "#"
  - name: Shalom Yiblet
    url: "#"
  - name: Danish Danish
    url: "http://cs.cmu.edu/~ddanish"

editors:
  - name: Maruan Al-Shedivat
    url: "https://www.cs.cmu.edu/~mshediva/"  # optional URL to the editor's homepage

abstract: >
  An example abstract block.
---


## Part 1 (to be completed by Ramesh)

Template text


## Part 2 (to be completed by Tze)

Template text


## Part 3 (to be completed by Shalom)

Template text


## Conditional Random Fields (CRFs)

In this section, we will first motivate CRFs and discuss how they overcome the shortcomings of HMMs. Second, we will briefly describe its inference and learning mechanism. Next, we will mention some of its applications, and empirical successes. Lastly, we will conclude with some of the recent research on hybrid neural-CRFs.

<figure>
  <div class="row">
    <div class="col three">
      <img src="{{ '/assets/img/notes/lecture-07/crfs.png' | relative_url }}" />
    </div>
  </div>
  <figcaption>
    <strong>Example of a 1-D chain CRF model.</strong>
  </figcaption>
</figure>

_**Introduction**_: The graphical representation of a 1-D chain CRF is depicted in the figure above. CRFs can be thought of as an evolution of HMMs, but unlike HMMs they are discriminative models, and directly
model the conditional distribution $P(\mathbf{Y}|\mathbf{X})$ instead of the joint distribution $P(\mathbf{X}, \mathbf{Y})$.
Most often, we care about the prediction anyway, so modelling it directly avoids the computation of the joint probability.

CRFs offer two key advantages over HMMs:

- They model the entire observed sequence. For example, in a task where we want to predict the Part-Of-Speech (POS) tags for a sequence of words in a sentence, it helps to know the entire sequence of words while predicting the POS tag at timestep $t$.
- They do not suffer from the label bias problem. We briefly discuss the problem below.

<figure>
  <div class="row">
    <div class="col two">
      <img src="{{ '/assets/img/notes/lecture-07/label_bias.png' | relative_url }}" />
    </div>
  </div>
  <figcaption>
    <strong>An example to demonstrate the label bias problem.</strong>
  </figcaption>
</figure>

*Label Bias Problem*: In the figure above, we can observe that state 1 transitions into only two states (state 1, and 2); whereas state 2 can potentially transition into all the five available states. Furthermore, we observe that at every time step, state 1 always prefers to transition to state 2 over state 1, and similarly state 2 always prefers to transition to state 2 over other 4 options. However, path ```1 -> 1 -> 1 -> 1``` is the most likely path with probability of $0.09$, whereas the *intuitively correct* path of ```1 -> 2 -> 2 -> 2``` has a probability mass of only $0.054$. The reason for this counter-intuitive phenomena is rooted in the fact that probability mass from state 2 is distributed across all states. Thus, the probabilities being locally normalized is not desirable here. CRFs overcome this problem by having *potential scores* instead of probabilities, which are *globally normalized*.

Having motivated why such models might be useful, let us see how one can learn and do inference over such models.

_**Inference and learning**_: The inference algorithm for CRFs is very simple, and just like HMMs, we can do Viterbi decoding.

The task of learning is essentially learning the weights ($\mathbf{w}$) for the features $\mathbf{f}(y_i, y_{i-1}, \mathbf{x}_{1:n})$ used to score the configurations. One can draw parallels here to a well known class of discriminative classifiers --- logistic regression, and just like logistic regression, we can do gradient ascent over the weights to increase the conditional probability over the observed set of $\mathbf{X}, \mathbf{Y}$ pairs. Given the 1-D chain structure, the normalizing term (or the partition function) can be tractably computed. In practice, gradient ascent has very slow convergence, and other alternatives like conjugate gradient method and limited memory quasi-newton methods can be used for faster learning.
Most CRFs, especially ones involving arbitrary structure, are intractable in general owing to the difficulty in computing the partition function.

_**Applications**_: CRFs have been very successful in a wide range of applications across different fields. In NLP, they are commonly used for POS-tagging, Named Entity Recognition (NER), and other tagging problems. In computer vision, they have found utility in image segmentation, image restoration, handwriting recognition among other tasks.

_**Recent developments**_: A popular recent trend in NLP tagging problems is to combine CRFs with neural representations from CNNs, and LSTMs instead of the hand-crafted features $\mathbf{f}(y_i, y_{i-1}, \mathbf{x}_{1:n})$, and the weights for the CNNs and LSTMs are also learnt as a part of the learning process (through back-propagation).

Lastly, a [tutorial](https://people.cs.umass.edu/~mccallum/papers/crf-tutorial.pdf) from Sutton, and McCallum is an excellent resource to know more in-depth details about CRFs